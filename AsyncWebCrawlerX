import requests
import threading
import time
from functools import wraps
from contextlib import contextmanager
from queue import Queue
from urllib.parse import urlparse
from bs4 import BeautifulSoup

class CrawlerMetrics:
    _instance = None
    
    def __new__(cls):
        if not cls._instance:
            cls._instance = super().__new__(cls)
            cls._instance.reset()
        return cls._instance
    
    def reset(self):
        self.crawled_count = 0
        self.error_count = 0
        self.start_time = time.time()

def rate_limiter(max_per_minute):
    min_interval = 60.0 / max_per_minute
    def decorator(func):
        last_called = 0.0
        @wraps(func)
        def wrapper(*args, **kwargs):
            nonlocal last_called
            elapsed = time.time() - last_called
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)
            last_called = time.time()
            return func(*args, **kwargs)
        return wrapper
    return decorator

@contextmanager
def timing_context(description):
    start = time.time()
    try:
        yield
    finally:
        print(f"{description} took {time.time() - start:.2f} seconds")

class CrawlerWorker(threading.Thread):
    def __init__(self, queue, metrics):
        super().__init__()
        self.queue = queue
        self.metrics = metrics
        self.daemon = True
    
    @rate_limiter(60)  # 60 requests per minute
    def fetch_url(self, url):
        try:
            with timing_context(f"Fetching {url}"):
                response = requests.get(url, timeout=5)
                response.raise_for_status()
                return response.text
        except Exception as e:
            print(f"Error fetching {url}: {str(e)}")
            self.metrics.error_count += 1
            return None
    
    def extract_links(self, html, base_url):
        soup = BeautifulSoup(html, 'html.parser')
        parsed_base = urlparse(base_url)
        links = set()
        for link in soup.find_all('a', href=True):
            href = link['href']
            parsed = urlparse(href)
            if not parsed.netloc:
                href = parsed_base._replace(path=parsed.path).geturl()
            links.add(href)
        return links
    
    def run(self):
        while True:
            url = self.queue.get()
            if url is None:
                break
            
            html = self.fetch_url(url)
            if html:
                self.metrics.crawled_count += 1
                links = self.extract_links(html, url)
                for link in links:
                    self.queue.put(link)
            self.queue.task_done()

class CrawlerManager:
    def __init__(self, initial_urls, num_workers=4):
        self.queue = Queue()
        self.metrics = CrawlerMetrics()
        self.workers = []
        
        for url in initial_urls:
            self.queue.put(url)
        
        for _ in range(num_workers):
            worker = CrawlerWorker(self.queue, self.metrics)
            self.workers.append(worker)
    
    def __enter__(self):
        for worker in self.workers:
            worker.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        for _ in self.workers:
            self.queue.put(None)
        for worker in self.workers:
            worker.join()
        
        print(f"\nCrawling complete!")
        print(f"Total crawled: {self.metrics.crawled_count}")
        print(f"Errors encountered: {self.metrics.error_count}")
        print(f"Total time: {time.time() - self.metrics.start_time:.2f} seconds")

def main():
    initial_urls = [
        'https://example.com',
        'https://www.wikipedia.org',
        'https://www.python.org'
    ]
    
    try:
        with CrawlerManager(initial_urls, num_workers=3) as manager:
            while True:
                time.sleep(1)
                qsize = manager.queue.qsize()
                print(f"\rQueue size: {qsize} | Crawled: {manager.metrics.crawled_count} | Errors: {manager.metrics.error_count}", end='')
                if qsize == 0:
                    break
    except KeyboardInterrupt:
        print("\nCrawling interrupted by user!")

if __name__ == "__main__":
    main()
